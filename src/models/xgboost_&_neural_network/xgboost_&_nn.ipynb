{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import util\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "import matplotlib.pylab as plt\n",
    "import h5py\n",
    "%matplotlib inline\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "seed = 25\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def print_metrics(qid,preds,truth):\n",
    "\n",
    "    print(\"ndcg10: {}\".format(util.get_ndcg(qid,preds,truth,k=10)))\n",
    "    print(\"ndcg5: {}\".format(util.get_ndcg(qid,preds,truth,k=5)))\n",
    "    print(\"map: {}\".format(util.get_mapk(qid, preds, truth)))\n",
    "    print(\"f1-micro: {}\".format(metrics.f1_score(y_true=truth,y_pred=preds,average=\"micro\")))\n",
    "    print(\"err@10: {}\".format(util.get_err(qid,preds,truth,k=10)))\n",
    "    print(\"err@5: {}\".format(util.get_err(qid,preds,truth,k=5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#load data\n",
    "\n",
    "#small\n",
    "train_small_df = pd.read_csv(\"../train_set_small_cleaned.csv\")\n",
    "test_small_df = pd.read_csv(\"../test_set_small_cleaned.csv\")\n",
    "\n",
    "#large\n",
    "train_large_df = pd.read_csv(\"../train_set_large_cleaned.csv\")\n",
    "test_large_df = pd.read_csv(\"../test_set_large_cleaned.csv\")\n",
    "\n",
    "#all\n",
    "train_all_df= pd.read_csv(\"../train_cleaned.csv\")\n",
    "test_all_df= pd.read_csv(\"../test_cleaned.csv\")\n",
    "\n",
    "train_small_df.drop(\"Unnamed: 0\",axis=1,inplace=True)\n",
    "train_large_df.drop(\"Unnamed: 0\",axis=1,inplace=True)\n",
    "train_all_df.drop(\"Unnamed: 0\",axis=1,inplace=True)\n",
    "test_small_df.drop(\"Unnamed: 0\",axis=1,inplace=True)\n",
    "test_large_df.drop(\"Unnamed: 0\",axis=1,inplace=True)\n",
    "test_all_df.drop(\"Unnamed: 0\",axis=1,inplace=True)\n",
    "\n",
    "\n",
    "#Prepare data for training\n",
    "#define features\n",
    "features = [x for x in train_small_df.columns if x not in [\"rel\",\"qid\"]]\n",
    "target =[\"rel\"]\n",
    "\n",
    "#scale features\n",
    "train_small_df[features] = StandardScaler(with_mean=0,with_std=1).fit_transform(train_small_df[features])\n",
    "test_small_df[features] = StandardScaler(with_mean=0,with_std=1).fit_transform(test_small_df[features])\n",
    "\n",
    "train_large_df[features] = StandardScaler(with_mean=0,with_std=1).fit_transform(train_large_df[features])\n",
    "test_large_df[features] = StandardScaler(with_mean=0,with_std=1).fit_transform(test_large_df[features])\n",
    "\n",
    "train_all_df[features] = StandardScaler(with_mean=0,with_std=1).fit_transform(train_all_df[features])\n",
    "test_all_df[features] = StandardScaler(with_mean=0,with_std=1).fit_transform(test_all_df[features])\n",
    "#reshape\n",
    "X_train_small = np.array(train_small_df[features])\n",
    "y_train_small = np.array(train_small_df[target])\n",
    "\n",
    "X_train_large = np.array(train_large_df[features])\n",
    "y_train_large = np.array(train_large_df[target])\n",
    "\n",
    "X_train_all = np.array(train_all_df[features])\n",
    "y_train_all = np.array(train_all_df[target])\n",
    "\n",
    "X_test_small = np.array(test_small_df[features])\n",
    "y_test_small = np.array(test_small_df[target])\n",
    "\n",
    "X_test_large = np.array(test_large_df[features])\n",
    "y_test_large = np.array(test_large_df[target])\n",
    "\n",
    "X_test_all = np.array(test_all_df[features])\n",
    "y_test_all = np.array(test_all_df[target])\n",
    "\n",
    "\n",
    "\n",
    "c, r = y_train_small.shape\n",
    "y_train_small = y_train_small.reshape(c,)\n",
    "\n",
    "c, r = y_test_small.shape\n",
    "y_test_small = y_test_small.reshape(c,)\n",
    "\n",
    "c, r = y_train_large.shape\n",
    "y_train_large = y_train_large.reshape(c,)\n",
    "\n",
    "c, r = y_test_large.shape\n",
    "y_test_large = y_test_large.reshape(c,)\n",
    "\n",
    "c, r = y_train_all.shape\n",
    "y_train_all = y_train_all.reshape(c,)\n",
    "\n",
    "c, r = y_test_all.shape\n",
    "y_test_all = y_test_all.reshape(c,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#===========Find best xgb model=============\n",
    "action = \"use_tuned_params\"   #possible actions: \"use_tuned_params\", \"tune_params\"\n",
    "\n",
    "\n",
    "#tune the hyperparameters | if false the already tuned parameters will be used\n",
    "if action == \"tune_params\":\n",
    "    n_estimators_range = [100,200,400]\n",
    "    learning_rate_range = [0.0001,0.01,0.1,1]\n",
    "    max_depth_range = [3,4,5,6,7]#list(range(3,10,2))\n",
    "    min_child_weight_range = [1,2,3,4,5,6]#[1,2,3,4,5,6]\n",
    "    scale_pos_weight_range = [0, 0.5, 1]\n",
    "    subsample_range = [0.5, 0.6 ,0.7]\n",
    "    colsample_bytree_range = [0.5, 0.6, 0.7]\n",
    "    reg_alpha_range = [105, 108,110,112, 115]\n",
    "\n",
    "\n",
    "    tuned_parameters = [{'max_depth': max_depth_range,\"learning_rate\":learning_rate_range,\"n_estimators\":n_estimators_range,\n",
    "                         \"gamma\":gamma_range,\"min_child_weight\":min_child_weight_range,\n",
    "                        \"subsample\":subsample_range,\"colsample_bytree\": colsample_bytree_range,\n",
    "                         \"scale_pos_weight\": scale_pos_weight_range,\"reg_alpha\":reg_alpha_range}]\n",
    "\n",
    "    scores=[\"f1_micro\"]\n",
    "    for score in scores:\n",
    "        xgb_model = GridSearchCV(XGBClassifier(objective=\"multi:softmax\",nthread=-1),param_grid=tuned_parameters,scoring=score,cv=3)\n",
    "        xgb_model.fit(X_train_small, y_train_small)\n",
    "\n",
    "        print(\"parameters selected: \",xgb_model.best_params_)\n",
    "        print(\" \")\n",
    "        print(\"Train set:\")\n",
    "        print(score+\":\",xgb_model.best_score_)\n",
    "        print(\" \")\n",
    "    #------------------------------------------------------------\n",
    "\n",
    "#use tuned parameters    \n",
    "elif action ==  \"use_tuned_params\":    \n",
    "    \n",
    "    n_estimators_range = [200]\n",
    "    learning_rate_range = [0.1]\n",
    "    max_depth_range = [3]\n",
    "    min_child_weight_range = [5]\n",
    "    gamma_range = [0]\n",
    "    scale_pos_weight_range = [0]\n",
    "    subsample_range = [0.6]\n",
    "    colsample_bytree_range = [0.7]\n",
    "    reg_alpha_range = [110]\n",
    "\n",
    "#define best found model tuned on the small dataset\n",
    "    xgb_model = XGBClassifier( objective=\"multi:softmax\",\n",
    "\n",
    "                        n_estimators=n_estimators_range[0], \n",
    "                         learning_rate=learning_rate_range[0],\n",
    "                        reg_alpha=reg_alpha_range[0],\n",
    "                        gamma= gamma_range[0],\n",
    "                        subsample= subsample_range[0],\n",
    "                        min_child_weight=min_child_weight_range[0],\n",
    "                        colsample_bytree=colsample_bytree_range[0],\n",
    "                        max_depth=max_depth_range[0],\n",
    "                        scale_pos_weight= gamma_range[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### XGBoost model - trained on small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ndcg10: 0.8246824680688686\n",
      "ndcg5: 0.8235922419976897\n",
      "map: 0.6576216627533964\n",
      "f1-micro: 0.58\n",
      "err@10: 0.32991239686000884\n",
      "err@5: 0.3078691262737157\n"
     ]
    }
   ],
   "source": [
    "load_pretrained_model = True\n",
    "\n",
    "if load_pretrained_model:\n",
    "    \n",
    "    filename_small = \"/home/andreas/Desktop/irdm/models/xgboost-trained_on_small.joblib.pkl\"\n",
    "    xgboost_small = joblib.load(filename_small)\n",
    "    print_metrics(test_small_df.qid,xgboost_small.predict(X_test_small),test_small_df.rel)\n",
    "    \n",
    "else:  \n",
    "    xgb_model.fit(X_train_small,y_train_small)\n",
    "    filename_small = \"/home/andreas/Desktop/irdm/models/xgboost-trained_on_small.joblib.pkl\"\n",
    "    _ = joblib.dump(xgb_model, filename_small, compress=9)\n",
    "    \n",
    "    xgboost_small = joblib.load(filename_small)\n",
    "    print_metrics(test_small_df.qid,xgboost_small.predict(X_test_small),test_small_df.rel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### XGBoost model - trained on large dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ndcg10: 0.8750850708775263\n",
      "ndcg5: 0.8805959601528669\n",
      "map: 0.6854773756434743\n",
      "f1-micro: 0.57875\n",
      "err@10: 0.4449925150616616\n",
      "err@5: 0.42641241170365807\n"
     ]
    }
   ],
   "source": [
    "load_pretrained_model = True\n",
    "\n",
    "if load_pretrained_model:\n",
    "    filename_large = \"/home/andreas/Desktop/irdm/models/xgboost-trained_on_large.joblib.pkl\"\n",
    "    xgboost_large = joblib.load(filename_large)\n",
    "    print_metrics(test_large_df.qid,xgboost_large.predict(X_test_large),test_large_df.rel)\n",
    "\n",
    "else:\n",
    "    xgb_model.fit(X_train_large,y_train_large)\n",
    "    filename_large = \"/home/andreas/Desktop/irdm/models/xgboost-trained_on_large.joblib.pkl\"\n",
    "    _ = joblib.dump(xgb_model, filename_large, compress=9)\n",
    "\n",
    "    xgboost_large = joblib.load(filename_large)\n",
    "    print_metrics(test_large_df.qid,xgboost_large.predict(X_test_large),test_large_df.rel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### XGBoost model - trained on all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ndcg10: 0.7433620383539687\n",
      "ndcg5: 0.7095928239953582\n",
      "map: 0.6885408057821056\n",
      "f1-micro: 0.5612141387291374\n",
      "err@10: 0.4601015817360638\n",
      "err@5: 0.44891153947187246\n"
     ]
    }
   ],
   "source": [
    "load_pretrained_model = True\n",
    "\n",
    "if load_pretrained_model:\n",
    "    filename_all = \"/home/andreas/Desktop/irdm/models/xgboost-trained_on_all.joblib.pkl\"\n",
    "    xgboost_all = joblib.load(filename_all)\n",
    "    print_metrics(test_all_df.qid,xgboost_all.predict(X_test_all),test_all_df.rel)\n",
    "\n",
    "else:\n",
    "    xgb.fit(X_train_all,y_train_all)\n",
    "    filename_all = \"/home/andreas/Desktop/irdm/models/xgboost-trained_on_all.joblib.pkl\"\n",
    "    _ = joblib.dump(xgb, filename_all, compress=9)\n",
    "\n",
    "    xgboost_all = joblib.load(filename_all)\n",
    "    print_metrics(test_all_df.qid,xgboost_all.predict(X_test_all),test_all_df.rel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#prepare data for neural network\n",
    "y_train_small = to_categorical(y_train_small, num_classes=None)\n",
    "y_test_small = to_categorical(y_test_small, num_classes=None)\n",
    "\n",
    "y_train_large = to_categorical(y_train_large, num_classes=None)\n",
    "y_test_large = to_categorical(y_test_large, num_classes=None)\n",
    "\n",
    "y_train_all = to_categorical(y_train_all, num_classes=None)\n",
    "y_test_all = to_categorical(y_test_all, num_classes=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Model\n",
    "epochs = 20\n",
    "\n",
    "model=keras.models.Sequential()\n",
    "model.add(Dense(activation=\"relu\",input_dim=136,units=700, kernel_initializer=\"TruncatedNormal\"))\n",
    "model.add(Dropout(rate=0.1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(activation=\"relu\",units=600,kernel_initializer=\"TruncatedNormal\"))\n",
    "model.add(Dropout(rate=0.3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(activation=\"relu\",units=450,kernel_initializer=\"TruncatedNormal\"))\n",
    "model.add(Dropout(rate=0.3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(activation=\"relu\",units=300,kernel_initializer=\"TruncatedNormal\"))\n",
    "model.add(Dropout(rate=0.25))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(activation=\"relu\",units=250,kernel_initializer=\"TruncatedNormal\"))\n",
    "model.add(Dropout(rate=0.25))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(activation=\"relu\",units=100,kernel_initializer=\"TruncatedNormal\"))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(activation=\"relu\",units=45,kernel_initializer=\"TruncatedNormal\"))\n",
    "model.add(Dropout(rate=0.1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(activation=\"softmax\",units=5,kernel_initializer=\"TruncatedNormal\"))\n",
    "\n",
    "    \n",
    "adam = keras.optimizers.SGD(nesterov=True)\n",
    "#adam = keras.optimizers.adam()\n",
    "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics= [\"accuracy\"])\n",
    "\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,patience=1, min_lr=0.001)\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss',patience=4,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### NN model - trained on small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ndcg10: 0.7705191639381013\n",
      "ndcg5: 0.7523945903295363\n",
      "map: 0.6516980250409213\n",
      "f1-micro: 0.58625\n",
      "err@10: 0.2554860929443916\n",
      "err@5: 0.24656176957935208\n"
     ]
    }
   ],
   "source": [
    "load_pretrained_model = True\n",
    "\n",
    "if load_pretrained_model:\n",
    "    filename_small_nn = \"/home/andreas/Desktop/irdm/models/nn-small.h5\"\n",
    "    nn_small = keras.models.load_model(filepath=filename_small_nn)\n",
    "    print_metrics(test_small_df.qid,nn_small.predict_classes(X_test_small,verbose=0),test_small_df.rel)\n",
    "\n",
    "else:\n",
    "\n",
    "    model.fit(X_train_small,y_train_small,batch_size=128,epochs=epochs,\\\n",
    "              validation_split=0,shuffle=True,validation_data=(X_test_small,y_test_small),\\\n",
    "              verbose=2,callbacks=[early_stopping])    \n",
    "\n",
    "    filename_small_nn = \"/home/andreas/Desktop/irdm/models/nn-small.h5\"\n",
    "    model.save(filepath=filename_small_nn)\n",
    "\n",
    "    nn_small = keras.models.load_model(filepath=filename_small_nn)\n",
    "    print_metrics(test_small_df.qid,nn_small.predict_classes(X_test_small,verbose=0),test_small_df.rel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### NN model - trained on large dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ndcg10: 0.8404642908974927\n",
      "ndcg5: 0.8429107121986785\n",
      "map: 0.6924643190012074\n",
      "f1-micro: 0.59125\n",
      "err@10: 0.43649615504114475\n",
      "err@5: 0.4218199224697093\n"
     ]
    }
   ],
   "source": [
    "load_pretrained_model = True\n",
    "\n",
    "if load_pretrained_model:\n",
    "    filename_large_nn = \"/home/andreas/Desktop/irdm/models/nn-large.h5\"\n",
    "    nn_large = keras.models.load_model(filepath=filename_large_nn)\n",
    "    print_metrics(test_large_df.qid,nn_large.predict_classes(X_test_large,verbose=0),test_large_df.rel)\n",
    "\n",
    "else:\n",
    "\n",
    "    model.fit(X_train_large,y_train_large,batch_size=128,epochs=epochs,\\\n",
    "              validation_split=0,shuffle=True,validation_data=(X_test_large,y_test_large),\\\n",
    "              verbose=2,callbacks=[early_stopping])    \n",
    "\n",
    "    filename_large_nn = \"/home/andreas/Desktop/irdm/models/nn-large.h5\"\n",
    "    model.save(filepath=filename_large_nn)\n",
    "\n",
    "    nn_large = keras.models.load_model(filepath=filename_large_nn)\n",
    "    print_metrics(test_large_df.qid,nn_large.predict_classes(X_test_large,verbose=0),test_large_df.rel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### NN model - trained on all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ndcg10: 0.8085064464336663\n",
      "ndcg5: 0.7833719857168615\n",
      "map: 0.6912292367507209\n",
      "f1-micro: 0.5560303244852415\n",
      "err@10: 0.43087259492275753\n",
      "err@5: 0.41675862303028366\n"
     ]
    }
   ],
   "source": [
    "load_pretrained_model = True\n",
    "\n",
    "if load_pretrained_model:\n",
    "    filename_all_nn = \"/home/andreas/Desktop/irdm/models/nn-all.h5\"\n",
    "    nn_all = keras.models.load_model(filepath=filename_all_nn)\n",
    "    print_metrics(test_all_df.qid,nn_all.predict_classes(X_test_all,verbose=0),test_all_df.rel)\n",
    "\n",
    "else:\n",
    "\n",
    "    model.fit(X_train_all,y_train_all,batch_size=128,epochs=epochs,\\\n",
    "              validation_split=0,shuffle=True,validation_data=(X_test_all,y_test_all),\\\n",
    "              verbose=0,callbacks=[early_stopping])    \n",
    "\n",
    "    filename_all_nn = \"/home/andreas/Desktop/irdm/models/nn-all.h5\"\n",
    "    model.save(filepath=filename_all_nn)\n",
    "\n",
    "    nn_all = keras.models.load_model(filepath=filename_all_nn)\n",
    "    print_metrics(test_all_df.qid,nn_all.predict_classes(X_test_all,verbose=0),test_all_df.rel)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
